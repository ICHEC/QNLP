\section{Introduction}
\label{sec:introduction}

This document details the current stage of development of the application as well as the QNLP library. The encoding strategy of basis words to a binary representation will be described, as well as how MPI was enabled for the QNLP library. The application was profiled to examine its performance and to identify bottlenecks. Appropriate optimisations were then applied to the bottlenecks and the application re-profiled. Scaling experiments were then conducted for the application.

Note, that for the purpose of this deliverable, we have restricted ourselves to the simplest non-trivial noun-verb-noun sentence structure, and simplified basis set onto which we perform the mapping. 

\subsection{Abstraction of DisCo algorithms: sentence similarity}
\label{sec:abstraction_of_disco_algorithms}
The reference implementation of the CSC sentence similarity algorithm for natural language processing as defined by the DisCo algorithms~\cite{Zeng_Coecke_2016, clark_coecke_sadrzadeh_2010, Coecke_Sadrzadeh_Clark_2010} on a quantum simulator was detailed in deliverable D1.1. The CSC sentence similarity algorithm builds upon the closest vector problem algorithm which was defined in deliverable D2.1, with the purpose of incorporating grammatical structure to the algorithm for computing the sentence meaning.

However, the CSC approach could not be directly implemented due to the limitations imposed by the encoding paradigm used. Instead, an alternative model is used. We follow a similar approach to \cite{clark_coecke_sadrzadeh_2010} for comparing sentence meanings, wherein we define a basis set, and map our sentences onto this basis to determine meanings. Using the methods of encoding and Hamming distance allows then to build meanings between the encoded corpus and respective test vector. This method is discussed in detail in D2.1, and we refer the reader to there for further details.

\subsection{End-to-end Application}
\label{sec:intro_end_to_end}
The current stage of the application's development now includes a fully automated end-to-end solution in Python. The standard Python layer is used for the pre-compute step, and the Python bindings for the QNLP library's C++ layer defines the simulator object and corresponding member functions which are used for the 'shot based' experiments that compute the 'closest vector' problem. In order to complete this automation, an encoding strategy of the basis words had to be developed. An overview of the end-to-end application is given in Section~\ref{sed:End-to-end_Application}.

\subsection{Basis Word Encoding Strategy}
To ensure an accurate representation of the corpus data using the proposed encoding scheme, we are required going forward to determine the basis elements ordering using a graph-theory approach. Further details of this will be discussed in Section~\ref{sec:basis_words}.

\subsection{MPI}
\label{sec:intro_MPI}
The QNLP library and subsequently the application have now been successfully implemented using MPI which allows for larger scale experiments. This has been implemented for both the C++ layer of the application and the Python bindings of that layer. Section~\ref{sec:MPI} details how MPI was implemented.

\subsection{Optimisations}
\label{sec:intro_optimisations}
After identifying the cause of the performance issues described in Section~\ref{sec:intro_application_profiling}, appropriate optimisations were considered. It was decided that addressing the scalability of the \textit{nCU} algorithm would be most beneficial to the application's performance. The optimisation procedure to realise these improvements for the subsequent scaling runs are discussed in detail in Section~\ref{sec:optimisations}. This work is currently in progress and is expected to be completed within the coming weeks.

\subsection{Application Profiling}
\label{sec:intro_application_profiling}
Section~\ref{sec:Application_Profiling} discusses how the performance of the application was analysed using timing instrumentation and also the Intel\textregistered Parallel Studio XE performance analysis tool suite. It was discovered that the application did not scale well. Due to the scaling issues discovered, a considerable amount of effort was spent in optimising the application. This involved finding the regions of code which were major contributors to the overhead using the profiling tools. The application was found to be MPI bound and the \textit{nCU} gate operations were observed to be the most significant contributor to this overhead. This was a significant factor which limited the scalability of the application. 

\subsection{Scaling of the Application}
\label{sec:intro_scaling}
Scaling experiments were conducted for the application and are discussed in Section~\ref{sec:Scaling_of_the_Application}. The application was shown not to scale well due to the large gate depth which increases significantly as the number of qubits in the system increases. It is expected that the application will scale better after the optimisations to reduce the gate depth discussed in Section~\ref{sec:intro_optimisations} are applied. It was noted for small problem sizes, the application scaled quite well as the number of compute resources increased. MPI communications were discovered not to be a limiting factor to the scalability for small problem sizes. Current work involves addressing these scalability issues. This is done by conducting an iterative approach to profiling, optimising and comparing performance.
