\section{End-to-end Application}
\label{sed:End-to-end_Application}
The current state of the application is a complete end-to-end implementation of the solution. This is executed using Python on the front-end while the quantum simulator executes using C++ on the back-end. Python is used to parse the initial data, conduct a pre-compute step, and produce graphical outputs of the results at the end of the computation. Given any corpus and a suitable basis set, the meaning of a test sentence relative to that corpus can be obtained by running the Python implementation of the application.

The Disco algorithms~\cite{Zeng_Coecke_2016, clark_coecke_sadrzadeh_2010, Coecke_Sadrzadeh_Clark_2010} and the 'closest vector problem' algorithm are the main components of the application. Each of these components have already been detailed in deliverables D1.1 and D2.1. How the software of each component integrates with each-other will be described below. 

\subsection{Application Overview}
\label{sec:Application_Overview}
The implementation of the algorithms can be separated into two main components: the distributed and compositional semantic algorithms, and the `closest vector problem' algorithm. These two components have been implemented in Python and C++ respectively as shown in Figure~\ref{fig:qnlp_controlflow}. To allow for a more seamless workflow and ease of use, the C++ layer of the application has been extended to Python using pybind11 as discussed in D2.1. Thus, the end-to-end application can be fully implemented using Python. Furthermore, the use of Python for the full application allows for subsequent quick and immediate analysis of the results in the form of the distribution of measured states.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{Images/QNLP_Controlflow.pdf}
    \caption{Control flow of QNLP project implementation including Python layer and C++ Layer which is also extended to Python.}
    \label{fig:qnlp_controlflow}
\end{figure}


\subsection{Python Layer}
\label{sec:Python_Layer}

The Python layer acts primarily as a pre-compute stage of the application. Upon parsing a corpus, the text must be simplified and punctuation removed. These processes are detailed in deliverables D1.1 and D2.1.

As mentioned in Section~\ref{sec:Application_Overview}, the distributional and compositional semantic algorithms are also applied during this stage. This pre-compute step is classical in nature and does not require the use of quantum resources. Due to the nature of experiments in quantum computing being 'shot based', the steps outlined in this Python layer are only computed once at the beginning of the computation and are not required to be re-computed at the beginning of each experiment.

Listed below are the steps taken in the Python layer of the application:
\begin{itemize}
    \item Corpus and basis sets are parsed
    \item Corpus and basis sets are formatted
    \item Unique binary encodings are generated for each basis word
    \item Distributed and compositional semantic algorithms are applied to corpus
    \item The corpus is now represented in terms of the binary encodings of the basis set. These binary vectors become our 'training' set
    \item The same is done for the test vector
\end{itemize}

\subsection{C++/Extended Python Layer}
\label{sec:cpp_extended_python_layer}

The C++/extended Python layer of the application handles the main component of the application which is also the most expensive in terms of computational resources. It computes the 'shot based' experiments and collects a distribution of measured states. The QNLP library can be compiled so that the Intel\textregistered-QS acts as the quantum simulator used in the backend. The QNLP library contains basic gate operations as well as more complicated and optimised routines such as the \textit{NCU} gate. On the higher level of these routines includes functions to encode states into a superposition, and to adjust the amplitudes of each state proportional to the Hamming distance between a test vector and superposition of training states.

This software layer of the application has also been successfully extended to Python using pybind11. Thus, all of the functionality of the quantum simulator being used in the back-end is available using Python. This allows for the entire application to be contained in Python making the workflow more integrated and user-friendly while not impacting performance.

Listed below are the steps taken in the C++/extended Python layer of the application:
\begin{itemize}
    \item The register object in the quantum simulator is constructed (allocating a large amount of memory) or re-initialised if already set.
    \item The binary representation of the corpus is encoded into a superposition of states.
    \item The test vector is loaded into a quantum register in each of the states.
    \item The 'closest vector problem' algorithm is applied to compare the test vector with the training vectors in each of the states. This adjusts the amplitude of each state proportionally to the binary difference between the two vectors.
    \item The qubits in the register representing the 'training' data are randomly collapsed, resulting in a measured state.
    \item This process (from the re-initialisation step on-wards) is repeated a number of times to generate a distribution of measured states. This distribution is proportional to the amplitudes of each of the states in the superposition.
    \item Finally, memory is de-allocated.
\end{itemize}

The output of the application is a distribution of the meaning space for the test vector used. The binary vectors are decoded to their word meanings so that interpretation of the results can be performed.

%\subsection{Distributed and Compositional Semantic Algorithms}
%\label{sec:Distributed_and_Compositional_Semantic_Algorithms}
%The distributed and compositional semantic algorithms are used to infer meaning from the sentences in a corpus by using word adjacency of corpus words to basis words that are also contained in the corpus, and also by taking grammatical structure into account. By applying these algorithms, each word in the corpus can be represented by a weighted sum of a number of basis words. For simplicity, it is assumed that the weights are equal in the current implementation. These algorithms are implemented in the Python layer of the application. 
%are detailed in D1.1.

%using the Intel\textregistered-QS using both Python and C++.