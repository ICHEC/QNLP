# Performance Optimisations: MPI Autotuning

This directory consists of multiple SLURM run scripts to generate MPI tuning parameter configuration files, and then to comparing the performances of each configuration file on a chosen application. 

Note: the below scripts assume that the SLURM job scheduler is being used and run scripts are launched using the `sbatch` command.

## Generating a MPI tuning parameter configuration file

Two scripts are provided in this directory to generate a single MPI tuning parameter configuration file;

- `run_generate_mpi_param_autotune.sh`  -   launch script for generic CPP executable
- `run_generate_mpi_param_autotune_EndToEndApp.sh` - launch script for QNLP end-to-end Python script (requires a command line argument in the form of a target corpus/text)

Both scripts launch parallel jobs across the specified job allocation. Parameters can be easily changed to launch application for any relevant executable/script.

Upon completion of the job, a configuration with a `.dat` extension will be generated (extension name can easily be changed in the script). This configuration file contains the optimal MPI communication subroutines to use.

## Running an application using an MPI tuning parameter configuration file

To run an application using the previously generated configuration file, simply export the following environment variable as the generated configuration file in the run script;

```{bash}
export I_MPI_TUNING_BIN=<GENERATED-CONFIG-FILE>
```

Examples of this can be observed in `run_compare_tuning_models.sh` and `run_compare_tuning_models_EndToEndApp.sh`.

## Performance comparison of different tuning configurations

The SLURM run scripts `run_compare_tuning_models.sh` and `run_compare_tuning_models_EndToEndApp.sh` can be used to compare the performance of different MPI tuning configurations

- `run_generate_mpi_param_autotune.sh`  -   launch script for generic CPP executable
- `run_generate_mpi_param_autotune_EndToEndApp.sh` - launch script for QNLP end-to-end Python script

Each of the scripts can be used for the target application type and their parameters can be adjusted as desired. 

Note that `run_generate_mpi_param_autotune_EndToEndApp.sh` requires at least one command line argument;

```{bash}
sbatch run_generate_mpi_param_autotune_EndToEndApp.sh <CORPUS-TEXT-FILE> <CONFIG-FILE-1> <CONFIG-FILE-2> ...
```

The first argument is non-optional and should be a file consisting of an example corpus to compute the QNLP application upon. Every subsequent arument is optional and should be a MPI tuning parameter configuration file.

The application will be run numerous times for each configuration file and the execution time will be printed and also outputted to a file specified in the respective job script. The default case of when `I_MPI_TUNING_BIN` will be the first instance of the application to be run. Followed by a run where `I_MPI_TUNING_BIN` set as follows;

```{bash}
export I_MPI_TUNING_BIN=${I_MPI_ROOT}/intel64/etc/tuning_skx_shm-ofi_efa.dat
```

This configuration file was generated during the installation stage of the Intel Parallel Studio Suite used. It was generated by the MPI Autotuning tool on a benchmark application.

After these executions, the application is rerun for each of the configuration files provided on the command line.

The execution time of each instance of the application run is outputted to a file and can be viewed to obtain the best performing configuration file. It is recommend to generate the configuration file on an application that displays similar or identical MPI communication patterns as the target application, along with a similar resource allocation as is expected to be used during production runs.

For the QNLP Python application, `${I_MPI_ROOT}/intel64/etc/tuning_skx_shm-ofi_efa.dat` consistently provided a significant performance benefit (~30-40%). Then a further smaller performance benefit was consistently observed using a configuration file generated on the same application and resource allocation (~5% further increase). Better performance benefits are expected for applications that display a larger proportion of collective MPI communication.